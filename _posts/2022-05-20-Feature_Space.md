---
layout: post
title: K近邻算法
date: 2022-05-20
Author: 乌墨_rfr
categories: 
tags: [machine-learning, Feature-Space]

comments: true





# 第一部分：K近邻算法原理

## **1.1、k近邻算法的基本概念，原理以及应用**

K近邻算法，即是给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例**最邻近**的K个实例，**这K个实例的多数属于某个类**，就把该输入实例分类到这个类中。（**这就类似于现实生活中少数服从多数的思想**）

![*k*近邻算法例子。测试样本（绿色圆形）应归入要么是第一类的蓝色方形或是第二类的红色三角形。如果k=3（实线圆圈）它被分配给第二类，因为有2个三角形和只有1个正方形在内侧圆圈之内。如果k=5（虚线圆圈）它被分配到第一类（3个正方形与2个三角形在外侧圆圈之内）](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/KnnClassification.svg/330px-KnnClassification.svg.png)



如上图所示，有**两类**不同的样本数据，分别用蓝色的小正方形和红色的小三角形表示，而图正中间的那个绿色的圆所标示的数据则是**待分类的数据**。这也就是我们的目的，来了一个新的数据点，我要得到它的类别是什么？好的，下面我们根据k近邻的思想来给绿色圆点进行分类。



- 如果K=3，绿色圆点的最邻近的3个点是2个红色小三角形和1个蓝色小正方形，**少数从属于多数，**基于统计的方法，判定绿色的这个待分类点属于红色的三角形一类。
- 如果K=5，绿色圆点的最邻近的5个邻居是2个红色三角形和3个蓝色的正方形，**还是少数从属于多数，**基于统计的方法，判定绿色的这个待分类点属于蓝色的正方形一类。

从上面例子我们可以看出，k近邻的算法思想非常的简单，也非常的容易理解，那么我们是不是就到此结束了，**该算法的原理我们也已经懂了，也知道怎么给新来的点如何进行归类，只要找到离它最近的k个实例，哪个类别最多即可。**



## **1.2、k近邻算法中k的选取以及特征归一化的重要性** 

### **1.选取k值以及它的影响**

k近邻的k值我们应该怎么选取呢？

**如果我们选取较小的k值，那么就会意味着我们的整体模型会变得复杂，容易发生过拟合！**恩~结论说完了，太抽象了吧你，不上图讲解号称通俗讲解的都是流氓**~好吧，那我就上图来讲解**

**假设我们选取k=1这个极端情况，怎么就使得模型变得复杂，又容易过拟合了呢？**

**假设我们有训练数据和待分类点如下图：**

![](https://pic2.zhimg.com/80/v2-6911dd1ce577c9fd6842cbd2ee68a309_720w.png)

图中有俩类，一个是**黑色的圆点**，一个是**蓝色的长方形**，现在我们的待分类点是**红色的五边形。**

好，根据我们的k近邻算法步骤来决定待分类点应该归为哪一类。我们由图中可以得到，**很容易我们能够看出来五边形离黑色的圆点最近，k又等于1，那太好了**，我们最终判定待分类点是黑色的圆点。

由这个处理过程我们很容易能够感觉出问题了，如果k太小了，比如等于1，那么模型就太复杂了，**我们很容易学习到噪声**，也就非常容易判定为噪声类别，而在上图，如果，k大一点，k等于8，**把长方形都包括进来**，我们很容易得到我们正确的分类应该是蓝色的长方形！如下图：

![](https://pic3.zhimg.com/80/v2-18df63acb37f29bd026e01770ef5c966_720w.png)

所谓的过拟合就是在训练集上准确率非常高，而在测试集上准确率低，经过上例，我们可以得到k太小会导致**过拟合**，**很容易将一些噪声（如上图离五边形很近的黑色圆点）学习到模型中，而忽略了数据真实的分布！**

**如果我们选取较大的k值，就相当于用较大邻域中的训练数据进行预测，这时与输入实例较远的（不相似）训练实例也会对预测起作用，使预测发生错误，k值的增大意味着整体模型变得简单。**

k值增大怎么就意味着模型变得简单了，不要急，我会解释的！哈哈。

**我们想，如果k=N（N为训练样本的个数）,那么无论输入实例是什么，都将简单地预测它属于在训练实例中最多的类。这时，模型是不是非常简单，这相当于你压根就没有训练模型呀！**直接拿训练数据统计了一下各个数据的类别，找最大的而已！这好像下图所示：

![](https://pic1.zhimg.com/80/v2-e79d46a56c426061a9494091f8fac068_720w.png)

我们统计了黑色圆形是8个，长方形个数是7个，那么哈哈，如果k=N，我就得出结论了，红色五边形是属于黑色圆形的（**明显是错误的好不，捂脸！**）



**这个时候，模型过于简单，完全忽略训练数据实例中的大量有用信息，是不可取的。**

**恩，k值既不能过大，也不能过小，在我举的这个例子中，我们k值的选择，在下图红色圆边界之间这个范围是最好的，如下图：**

![](https://pic3.zhimg.com/80/v2-b7dc18ee84e5c099c21fbaa175a7b9c6_720w.png)

**（注：这里只是为了更好让大家理解，真实例子中不可能只有俩维特征，但是原理是一样的1，我们就是想找到较好的k值大小）**

那么我们一般怎么选取呢？李航博士书上讲到，我们一般选取一个较小的数值，通常采取 交叉验证法来选取最优的k值。（**也就是说，选取k值很重要的关键是实验调参，类似于神经网络选取多少层这种，通过调整超参数来得到一个较好的结果**）

**小节**

如李航博士的一书「统计学习方法」上所说：

1. 如果选择较小的K值，就相当于用较小的领域中的训练实例进行预测，“学习”近似误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是“学习”的估计误差会增大，换句话说，K值的减小就意味着整体模型变得复杂，容易发生过拟合；

2. 如果选择较大的K值，就相当于用较大领域中的训练实例进行预测，其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大。这时候，与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误，且K值的增大就意味着整体的模型变得简单。

3. K=N，则完全不足取，因为此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的累，模型过于简单，忽略了训练实例中大量有用信息。

   

​        在实际应用中，K值一般取一个比较小的数值，例如采用 [交叉验证](http://zh.wikipedia.org/zh/交叉驗證)法（简单来说，就是一部分样本做训练集，一部分做测试集）来选择最优的K值。

###    **2.距离的度量**

在上文中说到，k近邻算法是在训练数据集中找到与该实例**最邻近**的K个实例，这K个实例的多数属于某个类，我们就说预测点属于哪个类。

定义中所说的最邻近是如何度量呢？我们怎么知道谁跟测试点最邻近。这里就会引出我们几种度量俩个点之间距离的标准。

我们可以有以下几种度量方式：



![img](https://pic1.zhimg.com/80/v2-60bb382b0d22ec0ce296ed0e024f31bc_720w.png)

其中当p=2的时候，就是我们最常见的欧式距离，我们也一般都用欧式距离来衡量我们高维空间中俩点的距离。在实际应用中，距离函数的选择应该根据数据的特性和分析的需要而定，一般选取p=2欧式距离表示。

其中具体包含的内容详细见[从K近邻算法、距离度量谈到KD树、SIFT+BBF算法](https://blog.csdn.net/v_july_v/article/details/8203674)

**恩，距离度量我们也了解了，下面我要说一下各个维度归一化的必要性！**

###   **3.特征归一化的必要性**

首先举例如下，我用一个人身高(cm)与脚码（尺码）大小来作为特征值，类别为男性或者女性。我们现在如果有5个训练样本，分布如下：

A [(179,42),男] B [(178,43),男] C [(165,36)女] D [(177,42),男] E [(160,35),女]

通过上述训练样本，我们看出问题了吗？

很容易看到第一维身高特征是第二维脚码特征的4倍左右，那么在进行距离度量的时候，**我们就会偏向于第一维特征。**这样造成俩个特征并不是等价重要的，最终可能会导致距离计算错误，从而导致预测错误。口说无凭，举例如下：

现在我来了一个测试样本 F(167,43)，让我们来预测他是男性还是女性，我们采取k=3来预测。

下面我们用欧式距离分别算出F离训练样本的欧式距离，然后选取最近的3个，多数类别就是我们最终的结果，计算如下：



![img](https://pic4.zhimg.com/80/v2-07d94c435dc95d66091768d56499f363_720w.png)

由计算可以得到，最近的前三个分别是C,D,E三个样本，那么由C,E为女性，D为男性，女性多于男性得到我们要预测的结果为**女性**。



**这样问题就来了，一个女性的脚43码的可能性，远远小于男性脚43码的可能性，那么为什么算法还是会预测F为女性呢？那是因为由于各个特征量纲的不同，在这里导致了身高的重要性已经远远大于脚码了，这是不客观的。**所以我们应该让每个特征都是同等重要的！这也是我们要归一化的原因！归一化公式如下：



![img](https://pic4.zhimg.com/80/v2-be30691d37ac93b2237217cadca2e967_720w.png)

讲到这里，k近邻算法基本内容我们已经讲完了。除去之后为了提高查找效率提出的kd树外，算法的原理，应用等方面已经讲解完毕，由于每篇文章内容不宜太多，kd树等知识下篇讲解，这里总结一下本文讲的内容。

## **1.3、第一部分小结**

1.我们提出了k近邻算法，算法的核心思想是，即是给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的K个实例，这K个实例的多数属于某个类，就把该输入实例分类到这个类中。**更通俗说一遍算法的过程，来了一个新的输入实例，我们算出该实例与每一个训练点的距离（这里的复杂度为0(n)比较大，所以引出了下文的kd树等结构），然后找到前k个，这k个哪个类别数最多，我们就判断新的输入实例就是哪类！**

2.与该实例最近邻的k个实例，这个最近邻的定义是通过不同**距离函数**来定义，我们最常用的是欧式距离。

3.为了保证每个特征同等重要性，我们这里对每个特征进行**归一化**。

4.k值的选取，既不能太大，也不能太小，何值为最好，需要实验调整参数确定！



# 第二部分：K近邻算法的实现：KD树

## 2.1、什么是KD树

Kd-树是K-dimension tree的缩写，是对数据点在k维空间（如二维(x，y)，三维(x，y，z)，k维(x1，y，z..)）中划分的一种数据结构，主要应用于多维空间关键数据的搜索（如：范围搜索和最近邻搜索）。本质上说，Kd-树就是一种平衡二叉树。

    首先必须搞清楚的是，k-d树是一种空间划分树，说白了，就是把整个空间划分为特定的几个部分，然后在特定空间的部分内进行相关搜索操作。
## 2.2、KD树基本原理

kd树构建的伪代码如下图所示（伪代码来自《图像局部不变特性特征与描述》王永明 王贵锦 编著）：

![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWctbXkuY3Nkbi5uZXQvdXBsb2Fkcy8yMDEyMTEvMjQvMTM1MzczMjA5MV80MjI1LmpwZw?x-oss-process=image/format,png)

再举一个简单直观的实例来介绍k-d树构建算法。假设有6个二维数据点{(2,3)，(5,4)，(9,6)，(4,7)，(8,1)，(7,2)}，数据点位于二维空间内，如下图所示。为了能有效的找到最近邻，k-d树采用分而治之的思想，即将整个空间划分为几个小部分，首先，粗黑线将空间一分为二，然后在两个子空间中，细黑直线又将整个空间划分为四部分，最后虚黑直线将这四部分进一步划分。

![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWctbXkuY3Nkbi5uZXQvdXBsb2Fkcy8yMDEyMTEvMjAvMTM1MzQwNTkyMV8zMDY2LmpwZw?x-oss-process=image/format,png)

6个二维数据点{(2,3)，(5,4)，(9,6)，(4,7)，(8,1)，(7,2)}构建kd树的具体步骤为：

确定：split域=x。具体是：6个数据点在x，y维度上的数据方差分别为39，28.63，所以在x轴上方差更大，故split域值为x；
确定：Node-data = （7,2）。具体是：根据x维上的值将数据排序，6个数据的中值(所谓中值，即中间大小的值)为7，所以Node-data域位数据点（7,2）。这样，该节点的分割超平面就是通过（7,2）并垂直于：split=x轴的直线x=7；
确定：左子空间和右子空间。具体是：分割超平面x=7将整个空间分为两部分：x<=7的部分为左子空间，包含3个节点={(2,3),(5,4),(4,7)}；另一部分为右子空间，包含2个节点={(9,6)，(8,1)}；

如上算法所述，kd树的构建是一个递归过程，我们对左子空间和右子空间内的数据重复根节点的过程就可以得到一级子节点（5,4）和（9,6），同时将空间和数据集进一步细分，如此往复直到空间中只包含一个数据点。

![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWctbXkuY3Nkbi5uZXQvdXBsb2Fkcy8yMDEyMTEvMjQvMTM1MzczNTQ2Ml80NTIyLmpwZw?x-oss-process=image/format,png)

与此同时，经过对上面所示的空间划分之后，我们可以看出，点(7,2)可以为根结点，从根结点出发的两条红粗斜线指向的(5,4)和(9,6)则为根结点的左右子结点，而(2,3)，(4,7)则为(5,4)的左右孩子(通过两条细红斜线相连)，最后，(8,1)为(9,6)的左孩子(通过细红斜线相连)。如此，便形成了下面这样一棵k-d树：

![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWctbXkuY3Nkbi5uZXQvdXBsb2Fkcy8yMDEyMTEvMjAvMTM1MzQwNjI3Nl80MDk1LmpwZw?x-oss-process=image/format,png)

k-d树的数据结构

![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWctbXkuY3Nkbi5uZXQvdXBsb2Fkcy8yMDEyMTEvMjAvMTM1MzQwNDY0OF8yMDg2LmpwZw?x-oss-process=image/format,png)

针对上表给出的kd树的数据结构，转化成具体代码如下所示(注，本文以下代码分析基于Rob Hess维护的sift库)：

```c++
/** a node in a k-d tree */
struct kd_node
{
	int ki;                      /**< partition key index *///关键点直方图方差最大向量系列位置
	double kv;                   /**< partition key value *///直方图方差最大向量系列中最中间模值
	int leaf;                    /**< 1 if node is a leaf, 0 otherwise */
	struct feature* features;    /**< features at this node */
	int n;                       /**< number of features */
	struct kd_node* kd_left;     /**< left child */
	struct kd_node* kd_right;    /**< right child */
};
```

也就是说，如之前所述，kd树中，kd代表k-dimension，每个节点即为一个k维的点。每个非叶节点可以想象为一个分割超平面，用垂直于坐标轴的超平面将空间分为两个部分，这样递归的从根节点不停的划分，直到没有实例为止。经典的构造k-d tree的规则如下：

1. 随着树的深度增加，循环的选取坐标轴，作为分割超平面的法向量。对于3-d tree来说，根节点选取x轴，根节点的孩子选取y轴，根节点的孙子选取z轴，根节点的曾孙子选取x轴，这样循环下去。
2. 每次均为所有对应实例的中位数的实例作为切分点，切分点作为父节点，左右两侧为划分的作为左右两子树。

![img](https://imgconvert.csdnimg.cn/aHR0cHM6Ly9pbWctbXkuY3Nkbi5uZXQvdXBsb2Fkcy8yMDEyMTEvMjQvMTM1MzczMDY5NF84OTQxLmdpZg)

 对于n个实例的k维数据来说，建立kd-tree的时间复杂度为O(k*n*logn)。

## 2.3、K树的构建及插入删除，改进

详细查看：

[从K近邻算法、距离度量谈到KD树、SIFT+BBF算法](https://blog.csdn.net/v_july_v/article/details/8203674)







参考：

李航博士《统计学习方法》

[一文搞懂k近邻（k-NN）算法（一）](https://zhuanlan.zhihu.com/p/25994179)

[【量化课堂】一只兔子帮你理解 kNN](https://link.zhihu.com/?target=https%3A//www.joinquant.com/post/2227%3Ff%3Dzh%26%3Bm%3D23028465)

[从K近邻算法、距离度量谈到KD树、SIFT+BBF算法 - 结构之法 算法之道 - 博客频道 - CSDN.NET](https://link.zhihu.com/?target=http%3A//blog.csdn.net/v_july_v/article/details/8203674)